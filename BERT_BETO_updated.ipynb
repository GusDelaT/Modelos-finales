{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Datos_Portadas.csv\")\n",
    "data = pd.DataFrame(df)\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_stopwords = [\n",
    "    \"de\", \"la\", \"que\", \"el\", \"en\", \"y\", \"a\", \"los\", \"del\", \"se\", \"las\",\n",
    "    \"por\", \"un\", \"para\", \"con\", \"no\", \"una\", \"su\", \"al\", \"es\", \"lo\",\n",
    "    \"como\", \"más\", \"pero\", \"sus\", \"le\", \"ya\", \"o\", \"fue\", \"me\", \"si\",\n",
    "    \"sin\", \"sobre\", \"este\", \"ya\", \"también\", \"entre\", \"cuando\", \"uno\", \"dos\",\n",
    "    \"tres\", \"cuatro\", \"cinco\", \"seis\", \"siete\", \"ocho\", \"nueve\", \"mas\", \"diez\", \"1\",\n",
    "    \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\",\"12\", \"13\", \"14\", \"15\", \"16\",\n",
    "    \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\",\n",
    "    \"aã\", \"tras\", \"mil\", \"anos\", \"tras\", \"niã\", \"son\", \"contra\", \"fueron\", \"hasta\",\n",
    "    \"queda\", \"hasta\", \"os\", \"sera\", \"durante\", \"van\", \"han\", \"q1\", \"q2\", \"q3\", \"q4\", \"q5\", \"q6\",\n",
    "    \"q7\", \"q8\", \"q9\", \"q10\", \"deja\", \"ante\", \"han\", \"estan\", \"pierde\", \"ha\", \"dia\", \"50\", \"2022\",\n",
    "    \"desde\", \"despues\", \"ano\", \"dias\", \"otra\", \"luego\", \"km\", \"pedro\", \"2024\", \"2025\", \"2023\", \"hace\",\n",
    "    \"donde\", \"otro\", \"daã\", \"iba\", \"les\", \"dan\", \"45\", \"tienen\", \"hacen\", \"juan\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=5000, stop_words=spanish_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with an empty string\n",
    "data[\"titulo\"] = data[\"titulo\"].fillna(\"\").astype(str)\n",
    "data[\"subtitular\"] = data[\"subtitular\"].fillna(\"\").astype(str)\n",
    "\n",
    "data[\"text\"] = data[\"titulo\"] + \" \" + data[\"subtitular\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Spanish stopwords (only needed once)\n",
    "#nltk.download(\"stopwords\")\n",
    "#spanish_stopwords = stopwords.words(\"spanish\")\n",
    "\n",
    "# Define range of parameters to test\n",
    "n_components_range = [5, 10, 15, 20]  # Example LDA n_components values\n",
    "num_classes_range = [5, 10, 15, 20]  # Example LCA num_classes values\n",
    "\n",
    "# Prepare an empty list to store the latent class distributions\n",
    "distributions = []\n",
    "\n",
    "for n_components in n_components_range:\n",
    "    for num_classes in num_classes_range:\n",
    "        # Apply LDA with Spanish stop words\n",
    "        vectorizer = CountVectorizer(max_features=5000, stop_words=spanish_stopwords)\n",
    "        X = vectorizer.fit_transform(data[\"text\"])\n",
    "\n",
    "        lda = LatentDirichletAllocation(n_components=n_components, random_state=42)\n",
    "        X_topics = lda.fit_transform(X)\n",
    "        data[\"topic\"] = X_topics.argmax(axis=1)\n",
    "\n",
    "        # Apply OneHotEncoder\n",
    "        encoder = OneHotEncoder(sparse_output=False)\n",
    "        X_lca = encoder.fit_transform(data[[\"topic\"]])\n",
    "\n",
    "        # Apply LCA (GaussianMixture)\n",
    "        lca_model = GaussianMixture(n_components=num_classes, covariance_type=\"full\", random_state=42)\n",
    "        data[\"latent_class\"] = lca_model.fit_predict(X_lca)\n",
    "\n",
    "        # Calculate class distribution\n",
    "        class_distribution = data[\"latent_class\"].value_counts().values\n",
    "        distributions.append([n_components, num_classes, class_distribution])\n",
    "\n",
    "# Convert results into a DataFrame for visualization\n",
    "distributions_df = pd.DataFrame(distributions, columns=[\"n_components\", \"num_classes\", \"class_distribution\"])\n",
    "\n",
    "# Make sure all classes are represented in each distribution\n",
    "distributions_df[\"class_count\"] = distributions_df[\"class_distribution\"].apply(lambda x: np.sum(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a broader range of random states (e.g., 20 values between 0 and 3000)\n",
    "random_state_range = np.linspace(0, 3000, dtype=int)\n",
    "\n",
    "# Convert class distributions into DataFrame\n",
    "plot_data = []\n",
    "\n",
    "for entry in distributions:\n",
    "    random_state, num_classes, class_distribution = entry  # Ensure distributions include random_state\n",
    "    for class_idx, count in enumerate(class_distribution):\n",
    "        plot_data.append([random_state, num_classes, class_idx, count])\n",
    "\n",
    "# Create DataFrame\n",
    "plot_df = pd.DataFrame(plot_data, columns=[\"random_state\", \"num_classes\", \"class_idx\", \"count\"])\n",
    "\n",
    "# Plot distributions for each num_classes set\n",
    "plt.figure(figsize=(36, 8))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "for idx, num_classes in enumerate(num_classes_range):\n",
    "    plt.subplot(1, len(num_classes_range), idx + 1)\n",
    "    subset = plot_df[plot_df[\"num_classes\"] == num_classes]\n",
    "    sns.lineplot(\n",
    "        x=\"random_state\", y=\"count\", hue=\"class_idx\", data=subset, marker=\"o\", palette=\"tab10\"\n",
    "    )\n",
    "    plt.title(f\"Num Classes: {num_classes}\")\n",
    "    plt.xlabel(\"Random State\")\n",
    "    plt.ylabel(\"Class Count\")\n",
    "    plt.legend(title=\"Latent Class\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bics = []\n",
    "aics = []\n",
    "class_range = range(2, 30)  # Try between 2 and 10 classes\n",
    "\n",
    "for n in class_range:\n",
    "    gmm = GaussianMixture(n_components=19, covariance_type=\"tied\", random_state=50)\n",
    "    gmm.fit(X_lca)\n",
    "    bics.append(gmm.bic(X_lca))\n",
    "    aics.append(gmm.aic(X_lca))\n",
    "\n",
    "bic_scores = class_range[np.argmin(bics)]\n",
    "print(f\"Optimal number of classes based on BIC: {bic_scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_lca_scaled = scaler.fit_transform(X_lca)\n",
    "\n",
    "# Define range of latent classes to test\n",
    "class_range = range(2, 30)  # Try between 2 and 30 classes\n",
    "\n",
    "# Store results\n",
    "aic_scores = []\n",
    "entropy_scores = []\n",
    "\n",
    "def entropy(probs):\n",
    "    return -np.sum(probs * np.log(probs + 1e-10), axis=1).mean()\n",
    "\n",
    "for n in class_range:\n",
    "    lca_model = GaussianMixture(n_components=n, covariance_type=\"spherical\", init_params=\"kmeans\", random_state=25)\n",
    "    lca_model.fit(X_lca_scaled)\n",
    "\n",
    "    # Compute AIC\n",
    "    aic_scores.append(lca_model.aic(X_lca_scaled))\n",
    "\n",
    "    # Compute Entropy\n",
    "    class_probs = lca_model.predict_proba(X_lca_scaled)\n",
    "    entropy_scores.append(entropy(class_probs))\n",
    "\n",
    "# Plot AIC and Entropy\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel(\"Number of Classes\")\n",
    "ax1.set_ylabel(\"AIC\", color=\"tab:blue\")\n",
    "ax1.plot(class_range, aic_scores, marker=\"o\", color=\"tab:blue\", label=\"AIC\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Entropy\", color=\"tab:red\")\n",
    "ax2.plot(class_range, entropy_scores, marker=\"s\", color=\"tab:red\", label=\"Entropy\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.title(\"AIC vs Entropy for Different Class Counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_lca_scaled = scaler.fit_transform(X_lca)\n",
    "\n",
    "# Define number of latent classes\n",
    "num_classes = 12\n",
    "\n",
    "# Fit LCA model using Gaussian Mixture Model (GMM)\n",
    "lca_model = GaussianMixture(n_components=num_classes, covariance_type=\"spherical\", init_params=\"kmeans\", random_state=25)\n",
    "data[\"latent_class\"] = lca_model.fit_predict(X_lca)\n",
    "\n",
    "# Check class distribution\n",
    "print(data[\"latent_class\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(probs):\n",
    "    return -np.sum(probs * np.log(probs + 1e-10), axis=1).mean()\n",
    "\n",
    "class_probs = lca_model.predict_proba(X_lca)\n",
    "print(\"LCA Model Entropy:\", entropy(class_probs))\n",
    "\n",
    "# Get AIC value\n",
    "aic_value = lca_model.aic(X_lca_scaled)\n",
    "print(f\"AIC: {aic_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=data[\"latent_class\"])\n",
    "plt.title(\"Distribution of Latent Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.groupby(\"latent_class\")[\"topic\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract top words per topic without duplication\n",
    "def get_top_words_unique(lda_model, feature_names, num_words=50):\n",
    "    topic_keywords = {}\n",
    "    used_keywords = set()  # Keep track of already used keywords\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        top_keywords = []\n",
    "        for i in topic.argsort()[-num_words:]:\n",
    "            keyword = feature_names[i]\n",
    "            if keyword not in used_keywords:\n",
    "                top_keywords.append(keyword)\n",
    "                used_keywords.add(keyword)\n",
    "        \n",
    "        # Ensure we have enough unique keywords, if not, add more from the remaining options\n",
    "        if len(top_keywords) < num_words:\n",
    "            for i in topic.argsort()[:-num_words-1:-1]:\n",
    "                keyword = feature_names[i]\n",
    "                if keyword not in used_keywords:\n",
    "                    top_keywords.append(keyword)\n",
    "                    used_keywords.add(keyword)\n",
    "        \n",
    "        topic_keywords[topic_idx] = top_keywords\n",
    "    return topic_keywords\n",
    "\n",
    "# Get words for each topic ensuring no duplication\n",
    "topic_keywords = get_top_words_unique(lda, feature_names)\n",
    "\n",
    "# Display top words per topic\n",
    "for topic, words in topic_keywords.items():\n",
    "    print(f\"Topic {topic}: {', '.join(words)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = lca_model.predict_proba(X_lca_scaled)\n",
    "print(probs.min(), probs.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esta es la conexion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_title = \"colisionan y vuelcan\"\n",
    "new_subtitle = \"familia se dirigia a vender al mercado\"\n",
    "\n",
    "# Merge text\n",
    "new_text = new_title + \" \" + new_subtitle\n",
    "\n",
    "# Transform into vector\n",
    "X_new = vectorizer.transform([new_text])\n",
    "X_new_topic = lda.transform(X_new).argmax(axis=1)  # Get topic\n",
    "\n",
    "# Convert topic into categorical format\n",
    "X_new_lca = encoder.transform([[X_new_topic[0]]])\n",
    "\n",
    "# Predict latent class\n",
    "predicted_class = lca_model.predict(X_new_lca)\n",
    "print(\"Predicted Latent Class:\", predicted_class[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5ebf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Define category-related keywords with weights\n",
    "category_keywords = {\n",
    "    \"Religión y Tradición\": [\"tradicional\", \"cortejo\", \"nazareno\", \"mayor\", \"comienzan\", \"independencia\", \"imagen\", \"acompanan\", \"conmemoran\", \"patrona\", \"feligreses\", \"cuaresma\", \"recorre\", \"concepcion\", \"catolicos\", \"historico\", \"recorrido\", \"domingo\", \"mercado\", \"cientos\", \"estudiantes\", \"procesion\", \"antigua\", \"festejos\", \"participan\", \"preparan\", \"devotos\", \"fiestas\", \"actividades\", \"visitantes\", \"virgen\", \"celebran\"],\n",
    "    \"Criminalidad y Violencia\": [\"cerca\", \"asesinato\", \"nina\", \"mecanico\", \"esperaron\", \"orfandad\", \"hermosa\", \"problema\", \"sicarios\", \"nino\", \"desconocidos\", \"atacan\", \"bala\", \"eeuu\", \"asesinan\", \"atacado\", \"balazos\", \"menor\"],\n",
    "    \"Accidentes Viales\": [\"dirigia\", \"embestido\", \"salio\", \"familiares\", \"milagro\", \"hija\", \"murio\", \"frenos\", \"fallecio\", \"salvan\", \"hijos\", \"salen\", \"barranco\", \"carretera\", \"morir\", \"quedo\", \"amigos\", \"auto\", \"madre\", \"frente\", \"hondonada\", \"arrollado\", \"iban\", \"carro\", \"impacta\", \"choca\", \"metros\", \"cae\"],\n",
    "    \"Proyectos de Infraestructura\": [\"santiago\", \"nuevo\", \"mareros\", \"fuerzas\", \"seguridad\", \"adultos\", \"herido\", \"mexico\", \"centro\", \"sale\", \"muerto\", \"hospital\", \"grave\", \"bus\", \"fiesta\", \"resultan\", \"victima\", \"regresaba\", \"heridos\", \"asalto\", \"pnc\", \"vehiculo\", \"sobrevive\", \"herida\", \"ser\", \"fuego\", \"accidente\", \"disparan\", \"mueren\", \"muere\", \"tragedia\", \"familia\", \"victimas\", \"armado\", \"ataque\"],\n",
    "    \"Demandas Sociales\": [\"quedar\", \"sur\", \"ayuda\", \"casco\", \"sigue\", \"casi\", \"tierra\", \"construir\", \"drenaje\", \"pavimentacion\", \"zonas\", \"cierre\", \"via\", \"urbano\", \"podria\", \"reparacion\", \"invierno\", \"viven\", \"transporte\", \"colapsar\", \"comuna\", \"tramo\", \"proyecto\", \"reparan\"],\n",
    "    \"Riesgos y Emergencias\": [\"afectadas\", \"provisional\", \"alternas\", \"perros\", \"afecta\", \"habilitan\", \"ocasiona\", \"vivienda\", \"punto\", \"complica\", \"vias\", \"villa\", \"ciudad\", \"hacia\", \"socavamiento\", \"carril\", \"barrio\", \"acceso\", \"derrumbe\", \"construccion\", \"dano\", \"desnivel\", \"hundimiento\", \"colapso\", \"kilometro\"],\n",
    "    \"Inseguridad y Conflictos\": [\"carreteras\", \"camino\", \"fin\", \"siguen\", \"temen\", \"recibir\", \"guatemala\", \"falta\", \"pueden\", \"esta\", \"cosechas\", \"autoridades\", \"primera\", \"chimaltenango\", \"debido\", \"lago\", \"muro\", \"dejan\", \"coatepeque\", \"riesgo\", \"aldeas\", \"partido\", \"pierden\", \"derrumbes\", \"agricultores\", \"incendio\", \"semana\", \"paso\", \"afectan\", \"locales\", \"transportistas\", \"perdidas\", \"oficial\", \"nacional\", \"municipal\", \"millones\", \"evitar\", \"vehicular\", \"conductores\", \"peten\", \"ruta\", \"piden\"],\n",
    "    \"Seguridad y Bienestar\": [\"viviendas\", \"lluvias\", \"costo\", \"guatemaltecos\", \"abandono\", \"rafael\", \"dios\", \"comerciantes\", \"seran\", \"comunidades\", \"vuelven\", \"jesus\", \"sufren\", \"todo\", \"salud\", \"casos\", \"fe\", \"vehiculos\", \"municipio\", \"reciben\", \"esperan\", \"centros\", \"menos\", \"personas\", \"ninos\", \"vendedores\", \"fieles\", \"devocion\", \"obra\", \"municipios\", \"horas\", \"llegar\", \"patrono\", \"agua\", \"antonio\", \"bloqueos\", \"trabajos\", \"flores\", \"familias\", \"miguel\", \"cada\", \"francisco\", \"afectados\", \"pobladores\", \"jose\", \"aldea\", \"vecinos\", \"marcos\", \"sacatepequez\", \"san\"],\n",
    "    \"Desastres Naturales\": [\"tendran\", \"cayo\", \"final\", \"hallado\", \"pueblo\", \"capital\", \"descenso\", \"nivel\", \"arriesgan\", \"quebrada\", \"rural\", \"vez\", \"habitantes\", \"lluvia\", \"tratamiento\", \"mundo\", \"precio\", \"area\", \"afluente\", \"desborde\", \"julia\", \"planta\", \"residentes\", \"caudal\", \"crecida\", \"rios\", \"motagua\"],\n",
    "    \"Accidentes y Tragedias\": [\"izabal\", \"puerto\", \"buscan\", \"tenia\", \"vecino\", \"agricultor\", \"despiden\", \"julio\", \"tormenta\", \"hectareas\", \"registran\", \"norte\", \"sido\", \"sepultan\", \"supuestos\", \"prevenir\", \"dejado\", \"mama\", \"ocurrido\", \"80\", \"joven\", \"pais\", \"forestales\", \"barrios\", \"departamento\", \"policia\", \"inundaciones\", \"asesinado\", \"incendios\", \"dengue\"],\n",
    "    \"Crimenes\": [\"lleva\", \"turistas\", \"servicio\", \"caminan\", \"clandestinos\", \"cementerio\", \"christi\", \"temporada\", \"desfile\", \"honor\", \"patronal\", \"comunidad\", \"ana\", \"lugar\", \"tiene\", \"material\", \"pasado\", \"limpian\", \"tradicion\", \"mal\", \"calles\", \"estado\", \"basureros\", \"llevan\", \"contaminacion\", \"atencion\", \"exigen\", \"lodo\", \"negro\", \"toneladas\", \"cristo\", \"desechos\", \"feria\", \"basura\"],\n",
    "    \"Educación y Salud Comunitaria\": [\"drenajes\", \"educativos\", \"solo\", \"labores\", \"instalaciones\", \"agresion\", \"sexual\", \"atienden\", \"docentes\", \"ninas\", \"colapsa\", \"negras\", \"aguas\", \"techo\", \"colision\", \"ciclo\", \"escolar\", \"pacientes\", \"aulas\", \"maestros\", \"padres\", \"alumnos\", \"clases\", \"escuela\"],\n",
    "}\n",
    "\n",
    "# Convert category keywords into a feature vector\n",
    "def get_weighted_features(text):\n",
    "    words = text.lower().split()\n",
    "    category_scores = {category: 0 for category in category_keywords}\n",
    "    \n",
    "    for category, keywords in category_keywords.items():\n",
    "        for word in words:\n",
    "            if word in keywords:\n",
    "                category_scores[category] += 1  # Assign a weight based on word presence\n",
    "    \n",
    "    return np.array(list(category_scores.values()))\n",
    "\n",
    "# Example usage\n",
    "new_title = \"\"\n",
    "new_subtitle = \"reparacion negro pacientes aldea puente paso arrollado hombre muere hospital\"\n",
    "new_text = new_title + \" \" + new_subtitle\n",
    "features = get_weighted_features(new_text)\n",
    "print(\"Feature Vector:\", features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29423332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Categorize New Texts\n",
    "\n",
    "def categorize_new_texts(new_data, vectorizer, lda_model, lca_model, encoder):\n",
    "    \"\"\"\n",
    "    Categorizes new newspaper texts based on trained LDA and LCA models.\n",
    "    \n",
    "    Parameters:\n",
    "    new_data (pd.DataFrame): A DataFrame with \"titulo\" and \"subtitular\" columns.\n",
    "    vectorizer (CountVectorizer): Trained CountVectorizer.\n",
    "    lda_model (LatentDirichletAllocation): Trained LDA model.\n",
    "    lca_model (GaussianMixture): Trained LCA model.\n",
    "    encoder (OneHotEncoder): Trained encoder for LCA.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The new data with assigned topic and latent class.\n",
    "    \"\"\"\n",
    "    # Ensure text columns are strings and concatenate\n",
    "    new_data[\"titulo\"] = new_data[\"titulo\"].fillna(\"\").astype(str)\n",
    "    new_data[\"subtitular\"] = new_data[\"subtitular\"].fillna(\"\").astype(str)\n",
    "    new_data[\"text\"] = new_data[\"titulo\"] + \" \" + new_data[\"subtitular\"]\n",
    "\n",
    "    # Transform text using the existing vectorizer\n",
    "    X_new = vectorizer.transform(new_data[\"text\"])\n",
    "    \n",
    "    # Predict topics using LDA\n",
    "    new_data[\"topic\"] = lda_model.transform(X_new).argmax(axis=1)\n",
    "\n",
    "    # Encode topic for LCA\n",
    "    X_new_encoded = encoder.transform(new_data[[\"topic\"]])\n",
    "    \n",
    "    # Predict latent classes using LCA\n",
    "    new_data[\"latent_class\"] = lca_model.predict(X_new_encoded)\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "# Load new data\n",
    "new_texts = pd.read_csv(\"categories.csv\")  # Path to your CSV file\n",
    "\n",
    "# Categorize new texts\n",
    "categorized_texts = categorize_new_texts(new_texts, vectorizer, lda, lca_model, encoder)\n",
    "\n",
    "# Print results\n",
    "print(categorized_texts[[\"titulo\", \"subtitular\", \"categoria\", \"topic\", \"latent_class\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d47f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inseguridad y Conflictos\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv('categories.csv')\n",
    "\n",
    "# View the first few rows of the dataset\n",
    "df.head()\n",
    "\n",
    "# Load BETO tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased', num_labels=len(df['categoria'].unique()))\n",
    "\n",
    "# Preprocess the title and subtitle into the format the model expects\n",
    "def preprocess_text(row):\n",
    "    # Ensure both title and subtitle are strings (handle NaN)\n",
    "    title = str(row['titulo']) if pd.notnull(row['titulo']) else ''\n",
    "    subtitle = str(row['subtitular']) if pd.notnull(row['subtitular']) else ''\n",
    "    \n",
    "    # Concatenate title and subtitle\n",
    "    text = title + \" \" + subtitle\n",
    "    return tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Apply preprocessing to your data\n",
    "df['inputs'] = df.apply(preprocess_text, axis=1)\n",
    "\n",
    "def classify_newspaper(text):\n",
    "    # Preprocess the input text (new title and subtitle)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Get the predicted category\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    return df['categoria'].unique()[predicted_class]\n",
    "\n",
    "# Test the classification with a new title and subtitle\n",
    "titulo = \"Carro choca en media via\"\n",
    "subtitulo = \"15 personas mueren en golpe de autobus luego de pasar por la carretera las fuentes\"\n",
    "new_text = titulo + \" \" + subtitulo\n",
    "print(classify_newspaper(new_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
